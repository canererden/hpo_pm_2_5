{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11649b41",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e5ca944",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PAKETLER\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.metrics import max_error, explained_variance_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, TimeDistributed, Bidirectional, ConvLSTM2D, Dropout\n",
    "from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c2cae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-28T22:53:14.743117Z",
     "iopub.status.busy": "2022-01-28T22:53:14.740119Z",
     "iopub.status.idle": "2022-01-28T22:53:14.754004Z",
     "shell.execute_reply": "2022-01-28T22:53:14.754004Z"
    },
    "executionInfo": {
     "elapsed": 692,
     "status": "ok",
     "timestamp": 1643404023507,
     "user": {
      "displayName": "Caner ERDEN",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhzPeinEBGlPvuQy-mLSs1nVsVu8vma6Q8f1ftyaQ=s64",
      "userId": "10580483392958313513"
     },
     "user_tz": -180
    },
    "id": "ixKsBn9aMe2O",
    "papermill": {
     "duration": 0.032855,
     "end_time": "2022-01-28T22:53:14.755003",
     "exception": false,
     "start_time": "2022-01-28T22:53:14.722148",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# değişecek parametreler\n",
    "model_number = str()\n",
    "learning_rate = 0.01\n",
    "dropout_rate = 0\n",
    "layer_size = 1\n",
    "units = 1\n",
    "activation_function = 'relu'\n",
    "loss_function = 'mae'\n",
    "epoch = 1\n",
    "batch_size = 48\n",
    "models = []\n",
    "optimizers = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bf2e46",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if optimizers == 'SGD':\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, clipnorm=1)\n",
    "elif optimizers == 'Adam':\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1)\n",
    "elif optimizers == 'Adamax':\n",
    "    optimizer = tf.keras.optimizers.Adamax(learning_rate=learning_rate, clipnorm=1)\n",
    "elif optimizers == 'Adagrad':\n",
    "    optimizer = tf.keras.optimizers.Adagrad(learning_rate=learning_rate, clipnorm=1)\n",
    "elif optimizers == 'RMSprop':\n",
    "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate, clipnorm=1)\n",
    "elif optimizers == 'Adadelta':\n",
    "    optimizer = tf.keras.optimizers.Adadelta(learning_rate=learning_rate, clipnorm=1)\n",
    "elif optimizers == 'Nadam':\n",
    "    optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate, clipnorm=1)\n",
    "elif optimizers == 'Ftrl':\n",
    "    optimizer = tf.keras.optimizers.Ftrl(learning_rate=learning_rate, clipnorm=1)\n",
    "# Değiştirilmeyen parametreler\n",
    "return_sequence = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99a39dd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[Buradan](https://medium.com/@tzjy/10-regression-metrics-data-scientist-must-know-tensorflow-keras-code-included-33f3ad53001c) alındı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a6db6",
   "metadata": {
    "code_folding": [
     483
    ],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 12345\n",
    "# Diğer Değişkenler\n",
    "target = 'PM2_5'\n",
    "n_hours = 24\n",
    "train_size = 0.7\n",
    "\n",
    "# Aynı sonuçlar üretebilmek için\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "print(\"SEED Ayarlandı: \", SEED)\n",
    "\n",
    "cols = [\n",
    "    \"PM2_5\",\n",
    "    \"SO2\",\n",
    "    \"NO\",\n",
    "    \"NO2\",\n",
    "    \"NOX\",\n",
    "    \"O3\",\n",
    "    \"wind_speed\",\n",
    "    \"relative_humidity\",\n",
    "    \"air_pressure\",\n",
    "]\n",
    "cols_to_analyze = [\n",
    "    \"PM2_5\",\n",
    "    \"SO2\",\n",
    "    \"NO\",\n",
    "    \"NO2\",\n",
    "    \"NOX\",\n",
    "    \"O3\",\n",
    "    \"temperature\",\n",
    "    \"wind_speed\",\n",
    "    \"relative_humidity\",\n",
    "    \"air_pressure\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# FONKSİYONLAR\n",
    "def get_data(data_name_xlsx):\n",
    "    raw_data = pd.read_excel(data_name_xlsx)\n",
    "    raw_data['date'] = pd.to_datetime(raw_data['date'],\n",
    "                                      format='%d.%m.%Y %H:%M')\n",
    "    raw_data.index = raw_data['date']\n",
    "    raw_data.drop('date', axis='columns', inplace=True)\n",
    "    raw_data_2015 = raw_data.loc[raw_data.index >= pd.to_datetime(\n",
    "        '01.01.2015 00:00', format='%d.%m.%Y %H:%M')]\n",
    "    return raw_data_2015\n",
    "\n",
    "\n",
    "def remove_outliers(dataset, cols):\n",
    "    # sıfırdan küçük olan aşağıdaki sütun değerlerini 0 yap.\n",
    "    # Sıfır değerlerini eksik veri olarak kaydet.\n",
    "    dataset[cols] = dataset[cols].clip(lower=0)\n",
    "    dataset[cols] = dataset[cols].replace(0, np.nan)\n",
    "    # rüzgar değişkenini kategorik veri yap KD: KuzeyDoğu\n",
    "    #wind_categories = pd.cut(\n",
    "    #    data.wind, bins=[-1, 90, 180, 270, 361], labels=[\"KD\", \"KB\", \"GB\", \"GD\"])\n",
    "    # data[\"wind_category\"] = wind_categories\n",
    "    # data.pop(\"wind\")\n",
    "    # dataset = pd.get_dummies(data, columns=[\"wind_category\"])\n",
    "\n",
    "    # Aykırı verileri de eksik veri olarak kaydet\n",
    "    for col in cols:\n",
    "        Q1 = dataset[col].quantile(0.25)\n",
    "        Q3 = dataset[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        # print(\"col: {}, Q1: {}, Q3: {}, IQR: {}\".format(col, Q1, Q3, IQR))\n",
    "        ust_limit = Q3 + 3 * IQR\n",
    "        alt_limit = Q1 - 3 * IQR\n",
    "        # print(\"ust_limit: {}, alt_limit: {}\".format(ust_limit, alt_limit))\n",
    "        dataset.loc[dataset[col] >= ust_limit, col] = np.nan\n",
    "        dataset.loc[dataset[col] <= alt_limit, col] = np.nan\n",
    "    dataset = dataset.replace(-9999, np.nan)\n",
    "    dataset['temperature'] = dataset['temperature'].replace(0, np.nan)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def impute_data(data):\n",
    "    filled_data = data.interpolate(method='linear', axis=0).ffill().bfill()\n",
    "    return filled_data\n",
    "\n",
    "\n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j + 1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "\n",
    "def scale_data(dataset):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_df = pd.DataFrame(scaler.fit_transform(dataset),\n",
    "                             index=dataset.index,\n",
    "                             columns=dataset.columns)\n",
    "    return scaler, scaled_df\n",
    "\n",
    "\n",
    "def create_dataset(data_name_xlsx, cols, n_hours, target, train_size):\n",
    "    \"\"\"\n",
    "    data_name_xlsx: Excel dosyasının adı\n",
    "    cols: özellikler listesi\n",
    "    n_hours: kaç saatlik veri girdi olarak verilecek\n",
    "    target: hangi özellik tahmin edilecek\n",
    "    train_size: eğitim oranı\n",
    "    \"\"\"\n",
    "    # load data\n",
    "    raw_data = get_data(data_name_xlsx)\n",
    "    # özellikler seti\n",
    "    raw_data = raw_data[cols]\n",
    "    # remove outliers\n",
    "    raw_data_missing = remove_outliers(raw_data, cols)\n",
    "    # raw_data_missing.to_csv(\"raw_data_missing.csv\")\n",
    "    # Eksik verilerin doldurulması\n",
    "    filled_data = impute_data(raw_data_missing)\n",
    "    filled_data.to_csv(\"data/filled_data.csv\")\n",
    "    # scaling\n",
    "    scaler, scaled_df = scale_data(filled_data)\n",
    "    # Veri çerçeveleme\n",
    "    reframed_df = pd.DataFrame(\n",
    "        series_to_supervised(scaled_df.to_numpy(), n_hours, len([target])))\n",
    "    # Düşürülecek sütunlar sondan özellik sayısından 1 eksik olana kadar gideceğiz.\n",
    "    reframed_df = reframed_df.iloc[:, :-(len(cols) - 1)]\n",
    "    reframed_df.index = filled_data.index[n_hours:]\n",
    "    # reframed_df.to_csv(\"reframed.csv\")\n",
    "    # split_data\n",
    "    X = reframed_df.iloc[:, :-1]\n",
    "    y = reframed_df.iloc[:, -1]\n",
    "    print(X.shape, \"x\", y.shape, \"y\")\n",
    "    X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(\n",
    "        X, y, train_size=train_size, shuffle=False)\n",
    "    X_train = X_train_df.to_numpy().reshape(\n",
    "        (X_train_df.shape[0], n_hours, scaled_df.shape[1]))\n",
    "    X_test = X_test_df.to_numpy().reshape(\n",
    "        (X_test_df.shape[0], n_hours, scaled_df.shape[1]))\n",
    "    print(X_train.shape, \"X_train.shape\")\n",
    "    print(X_test.shape, \"X_test.shape\")\n",
    "    print(y_train_df.shape, \"y_train_df.shape\")\n",
    "    print(y_test_df.shape, \"y_test_df.shape\")\n",
    "    return X_train, X_test, y_train_df, y_test_df\n",
    "\n",
    "\n",
    "def fit_model(layer_size, X_train, X_test, y_train_df, batch_size, epoch,\n",
    "              units, activation_function, loss_function, optimizer, layer):\n",
    "    if layer_size == 1:  # 1 gizli katman varsa\n",
    "        model = Sequential()\n",
    "        if layer[0] == 'GRU':\n",
    "            model.add(\n",
    "                GRU(units,\n",
    "                    activation=activation_function,\n",
    "                    input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif layer[0] == 'LSTM':\n",
    "            model.add(\n",
    "                LSTM(units,\n",
    "                     activation=activation_function,\n",
    "                     input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif layer[0] == 'SimpleRNN':\n",
    "            model.add(\n",
    "                SimpleRNN(units,\n",
    "                          activation=activation_function,\n",
    "                          input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "\n",
    "    elif layer_size == 2:  # 2 gizli katman varsa\n",
    "        model = Sequential()\n",
    "        first_layer = layer[0][0]\n",
    "        second_layer = layer[0][1]\n",
    "        if (first_layer == 'GRU') and (second_layer == 'GRU'):\n",
    "            print(\"GRU GRU\")\n",
    "            model.add(\n",
    "                GRU(units,\n",
    "                    activation=activation_function,\n",
    "                    input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                    return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(GRU(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif (first_layer == 'LSTM') and (second_layer == 'LSTM'):\n",
    "            model.add(\n",
    "                LSTM(units,\n",
    "                     activation=activation_function,\n",
    "                     input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                     return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(LSTM(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif (first_layer == 'SimpleRNN') and (second_layer == 'SimpleRNN'):\n",
    "            model.add(\n",
    "                SimpleRNN(units,\n",
    "                          activation=activation_function,\n",
    "                          input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                          return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(SimpleRNN(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif (first_layer == 'GRU') and (second_layer == 'LSTM'):\n",
    "            model.add(\n",
    "                GRU(units,\n",
    "                    activation=activation_function,\n",
    "                    input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                    return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(LSTM(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif (first_layer == 'GRU') and (second_layer == 'SimpleRNN'):\n",
    "            model.add(\n",
    "                GRU(units,\n",
    "                    activation=activation_function,\n",
    "                    input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                    return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(SimpleRNN(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif (first_layer == 'LSTM') and (second_layer == 'SimpleRNN'):\n",
    "            model.add(\n",
    "                LSTM(units,\n",
    "                     activation=activation_function,\n",
    "                     input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                     return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(SimpleRNN(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif (first_layer == 'LSTM') and (second_layer == 'GRU'):\n",
    "            model.add(\n",
    "                LSTM(units,\n",
    "                     activation=activation_function,\n",
    "                     input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                     return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(GRU(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif (first_layer == 'SimpleRNN') and (second_layer == 'GRU'):\n",
    "            model.add(\n",
    "                SimpleRNN(units,\n",
    "                          activation=activation_function,\n",
    "                          input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                          return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(GRU(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif (first_layer == 'SimpleRNN') and (second_layer == 'LSTM'):\n",
    "            model.add(\n",
    "                SimpleRNN(units,\n",
    "                          activation=activation_function,\n",
    "                          input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                          return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(LSTM(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "\n",
    "    else:  # 4 gizli katman varsa\n",
    "        model = Sequential()\n",
    "        if layer[0] == 'GRU':\n",
    "            model.add(\n",
    "                GRU(units,\n",
    "                    activation=activation_function,\n",
    "                    input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                    return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(GRU(units, activation=activation_function, return_sequences=True))\n",
    "            model.add(GRU(units, activation=activation_function, return_sequences=True))\n",
    "            model.add(GRU(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif layer[0] == 'LSTM':\n",
    "            model.add(\n",
    "                LSTM(units,\n",
    "                     activation=activation_function,\n",
    "                     input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                     return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(LSTM(units, activation=activation_function, return_sequences=True))\n",
    "            model.add(LSTM(units, activation=activation_function, return_sequences=True))\n",
    "            model.add(LSTM(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif layer[0] == 'SimpleRNN':\n",
    "            model.add(\n",
    "                SimpleRNN(units,\n",
    "                          activation=activation_function,\n",
    "                          input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                          return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(SimpleRNN(units, activation=activation_function, return_sequences=True))\n",
    "            model.add(SimpleRNN(units, activation=activation_function, return_sequences=True))\n",
    "            model.add(SimpleRNN(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=loss_function, optimizer=optimizer)\n",
    "    history = model.fit(X_train,\n",
    "                        y_train_df.to_numpy(),\n",
    "                        epochs=epoch,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(X_test, y_test_df.to_numpy()),\n",
    "                        verbose=1,\n",
    "                        shuffle=False)\n",
    "    return history, model\n",
    "\n",
    "\n",
    "def plot_history(history, title):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, \"b\", label=\"Train\")\n",
    "    plt.plot(epochs, val_loss, \"r\", label=\"Test\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"images/history{}.jpg\".format(model_number))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_true_preds(true_value, predicted_value, train_test):\n",
    "    # train_test train için mi test için mi\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(true_value, predicted_value, c='crimson')\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    p1 = max(max(predicted_value), max(true_value))\n",
    "    p2 = min(min(predicted_value), min(true_value))\n",
    "    plt.plot([p1, p2], [p1, p2], 'b-')\n",
    "    plt.xlabel('True Values', fontsize=15)\n",
    "    plt.ylabel('Predictions', fontsize=15)\n",
    "    plt.axis('equal')\n",
    "    plt.savefig(\"images/true_preds{}_{}.jpg\".format(model_number, train_test))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_model(filled_data, target, model, X_train, X_test, y_test_df,\n",
    "                   y_train_df):\n",
    "    scaler_pred = MinMaxScaler()\n",
    "    scaler_pred.fit(filled_data[target].to_numpy().reshape(-1, 1))\n",
    "    # make predictions (test)\n",
    "    y_pred_test = model.predict(\n",
    "        X_test)  # yhat tahmin değerleri y_test ler ile kıyaslanacak.\n",
    "    # inverse scaling for forecast\n",
    "    np.savetxt(\"y_pred_test.csv\", y_pred_test, delimiter = \",\")\n",
    "    print(\"y_pred_test shape\", y_pred_test.shape)\n",
    "    inv_y_pred_test = scaler_pred.inverse_transform(y_pred_test)\n",
    "    y_true_test = scaler_pred.inverse_transform(y_test_df.to_numpy().reshape(\n",
    "        -1, 1))\n",
    "    # make predictions (train)\n",
    "    y_pred_train = model.predict(\n",
    "        X_train)  # yhat tahmin değerleri y_test ler ile kıyaslanacak.\n",
    "    # inverse scaling for forecast\n",
    "    inv_y_pred_train = scaler_pred.inverse_transform(y_pred_train)\n",
    "    y_true_train = scaler_pred.inverse_transform(y_train_df.to_numpy().reshape(\n",
    "        -1, 1))\n",
    "\n",
    "    print(\"inv_y_pred_train dtype: \", inv_y_pred_train.dtype)\n",
    "    print(\"y_true_train dtype: \", y_true_train.dtype)\n",
    "    print(\"inv_y_pred_test dtype: \", inv_y_pred_test.dtype)\n",
    "    print(\"y_true_test dtype: \", y_true_test.dtype)\n",
    "\n",
    "    # TEST\n",
    "    test_mse = mean_squared_error(y_true_test, inv_y_pred_test)\n",
    "    test_mae = mean_absolute_error(y_true_test, inv_y_pred_test)\n",
    "    test_rmse = mean_squared_error(y_true_test, inv_y_pred_test) ** 0.5\n",
    "    test_r2 = r2_score(y_true_test, inv_y_pred_test)\n",
    "    test_mape = mean_absolute_percentage_error(y_true_test, inv_y_pred_test)\n",
    "    test_ME = max_error(y_true_test, inv_y_pred_test)\n",
    "    test_evs = explained_variance_score(y_true_test, inv_y_pred_test)\n",
    "\n",
    "    list_of_rows = []\n",
    "    test_perf = pd.Series({\n",
    "        'Set': 'test',\n",
    "        'MSE': test_mse,\n",
    "        'MAE': test_mae,\n",
    "        'RMSE': test_rmse,\n",
    "        'R2': test_r2,\n",
    "        'MAPE': test_mape,\n",
    "        'MAX_ERROR': test_ME,\n",
    "        'EXP_VAR_SCORE': test_evs\n",
    "    })\n",
    "    list_of_rows.append(test_perf)\n",
    "    # TRAIN\n",
    "\n",
    "\n",
    "\n",
    "    train_mse = mean_squared_error(y_true_train, inv_y_pred_train)\n",
    "    train_mae = mean_absolute_error(y_true_train, inv_y_pred_train)\n",
    "    train_rmse = mean_squared_error(y_true_train, inv_y_pred_train) ** 0.5\n",
    "    train_r2 = r2_score(y_true_train, inv_y_pred_train)\n",
    "    train_mape = mean_absolute_percentage_error(y_true_train, inv_y_pred_train)\n",
    "    train_ME = max_error(y_true_train, inv_y_pred_train)\n",
    "    train_evs = explained_variance_score(y_true_train, inv_y_pred_train)\n",
    "\n",
    "    train_perf = pd.Series({\n",
    "        'Set': 'train',\n",
    "        'MSE': train_mse,\n",
    "        'MAE': train_mae,\n",
    "        'RMSE': train_rmse,\n",
    "        'R2': train_r2,\n",
    "        'MAPE': train_mape,\n",
    "        'MAX_ERROR': train_ME,\n",
    "        'EXP_VAR_SCORE': train_evs\n",
    "    })\n",
    "    list_of_rows.append(train_perf)\n",
    "\n",
    "    df_results = pd.DataFrame(list_of_rows)\n",
    "    return df_results, inv_y_pred_test, y_true_test, inv_y_pred_train, y_true_train\n",
    "\n",
    "\n",
    "# Verilerin hazırlanması\n",
    "X_train, X_test, y_train_df, y_test_df = create_dataset(\"data/kagithane.xlsx\",\n",
    "                                                        cols=cols_to_analyze,\n",
    "                                                        n_hours=n_hours,\n",
    "                                                        target=target,\n",
    "                                                        train_size=train_size)\n",
    "# VERİ SETİ\n",
    "filled_data = pd.read_csv('data/filled_data.csv',\n",
    "                          index_col='date',\n",
    "                          parse_dates=True)\n",
    "\n",
    "# EĞİTİM\n",
    "print(\"Eğitim Başlatılıyor\")\n",
    "start_time = datetime.now()\n",
    "# Öğrenme Aşaması\n",
    "history1, model1 = fit_model(layer_size, X_train, X_test, y_train_df,\n",
    "                             batch_size, epoch, units, activation_function,\n",
    "                             loss_function, optimizer, models)\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))\n",
    "print(\"Eğitim Bitti. Model Kaydediliyor\")\n",
    "\n",
    "# EVALUATION\n",
    "df_results, inv_y_pred_test, y_true_test, inv_y_pred_train, y_true_train = evaluate_model(\n",
    "    filled_data, target, model1, X_train, X_test, y_test_df, y_train_df)\n",
    "\n",
    "first_date = pd.to_datetime('2015-01-02 00:00:00')\n",
    "row_size = filled_data.shape[0] - n_hours\n",
    "\n",
    "train_number = int(row_size * (train_size))\n",
    "test_number = row_size - train_number\n",
    "\n",
    "first_date = filled_data.index[n_hours]\n",
    "testin_basladigi_tarih = first_date + pd.to_timedelta(train_number, unit='h')\n",
    "\n",
    "test_indeksler = filled_data.loc[testin_basladigi_tarih:].index\n",
    "train_indeksler = filled_data.loc[first_date:(\n",
    "        testin_basladigi_tarih - pd.to_timedelta(1, unit='h'))].index\n",
    "\n",
    "test_true_pred_df = pd.DataFrame(\n",
    "    {\n",
    "        'y_true_test': y_true_test.flatten(),\n",
    "        'y_pred_test': inv_y_pred_test.flatten()\n",
    "    },\n",
    "    index=test_indeksler)\n",
    "train_true_pred_df = pd.DataFrame(\n",
    "    {\n",
    "        'y_true_train': y_true_train.flatten(),\n",
    "        'y_pred_train': inv_y_pred_train.flatten()\n",
    "    },\n",
    "    index=train_indeksler)\n",
    "\n",
    "# SAVINGS\n",
    "# save results to folder\n",
    "df_results.to_csv(\"performances/df_results{}.csv\".format(model_number))\n",
    "test_true_pred_df.to_csv(\n",
    "    \"predictions/test_true_pred{}.csv\".format(model_number))\n",
    "train_true_pred_df.to_csv(\n",
    "    \"predictions/train_true_pred{}.csv\".format(model_number))\n",
    "\n",
    "# Save Model\n",
    "model_json = model1.to_json()\n",
    "with open(\"models/model{}.json\".format(model_number), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model1.save_weights(\"models/model{}.h5\".format(model_number))\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# save history\n",
    "print(\"history saved\")\n",
    "hist_df = pd.DataFrame(history1.history)\n",
    "hist_csv_file = 'histories/history{}.csv'.format(model_number)\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)\n",
    "\n",
    "# PLOT RESULTS\n",
    "plot_true_preds(y_true_test, inv_y_pred_test, \"test\")\n",
    "plot_true_preds(y_true_train, inv_y_pred_train, \"train\")\n",
    "\n",
    "plot_history(history1, \"Training and Test Loss {}\".format(model_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aa374e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86405c61",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "kagithane_atm_met.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a4ae173dc0bc553dffd3337b933bf6f05eed740245579b100d876076ba6fb9f0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 505.912157,
   "end_time": "2022-01-28T23:01:31.746331",
   "environment_variables": {},
   "exception": true,
   "input_path": "input.ipynb",
   "output_path": "output_1.ipynb",
   "parameters": {
    "learning_rate": 0.1,
    "model_number": 1
   },
   "start_time": "2022-01-28T22:53:05.834174",
   "version": "2.3.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 567.841,
   "position": {
    "height": "40px",
    "left": "1010.09px",
    "right": "20px",
    "top": "101px",
    "width": "276px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}